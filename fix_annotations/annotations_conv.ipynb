{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6e4bf9d",
   "metadata": {},
   "source": [
    "# Edit Annotations\n",
    "\n",
    "Thanks for using this jupyter notebook. This notebook covers several useful functions to play around with the annotations we have. The list below summarises where you can find codes for each function.\n",
    "\n",
    "### **[Almost always needed]** Importing: \n",
    "Import JSON into DataFrame - Function (1)\n",
    "\n",
    "### Conversions:\n",
    "* Convert output from Detectron2 to human readable format (also for editing in CVAT) - Function (1a)\n",
    "* Convert from COCO json to YOLOv7 txt input - Function (3)\n",
    "* Convert from YOLOv7 txt output to COCO json - Function (4)\n",
    "* Convert two jsons into one - Function (5)\n",
    "\n",
    "### Queries and counting: *[in anntoations.ipynb]*\n",
    "\n",
    "~~* Count the number of instances of a certain category in the json - Function (2a)~~\n",
    "\n",
    "~~* Count the category (and/or clicks) distribution - Function (2)~~\n",
    "\n",
    "~~* Move wanted images indicated in a json from a library of images - Function (2b)~~\n",
    "\n",
    "### Special tasks: *[in annotations.ipynb]*\n",
    "\n",
    "~~* Trim a selection of annotation from a master json - Function (6)~~\n",
    "\n",
    "~~* create json for the split subset (for training, validation etc..)~~\n",
    "\n",
    "~~* create empty json for the split subset (let Detectron do its job)~~\n",
    "\n",
    "~~* create json for the remainders of the split~~\n",
    "\n",
    "~~* Clean up the json of the validation set - Function (7)~~"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08bc5602-b28f-46ad-82f0-ab422e61669f",
   "metadata": {},
   "source": [
    "# 1) Import JSON into DataFrame\n",
    "\n",
    "Export annotations from CVAT as COCO JSON. \n",
    "\n",
    "Put the json file in the working directory and change the variable \"filename\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "317dede2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# utilities\n",
    "\n",
    "def findCategory(data):\n",
    "    # find categories\n",
    "    cats = data[\"categories\"]\n",
    "    category = pd.DataFrame(cats)\n",
    "    category = category.drop(['supercategory'], axis=1)\n",
    "    category = category.rename(columns={'id': 'category_id'})\n",
    "    return category\n",
    "\n",
    "def findImages(data):\n",
    "    img = data[\"images\"]\n",
    "    images = pd.DataFrame(img)\n",
    "    \n",
    "    # unwanted columns exist if exported from CVAT. Not if generated by my code\n",
    "    if set(['license','flickr_url','coco_url','date_captured']).issubset(images.columns):\n",
    "        images = images.drop(columns=['license','flickr_url','coco_url','date_captured'])\n",
    "    \n",
    "    return images\n",
    "\n",
    "def findAnnotations(data):\n",
    "    anno = data[\"annotations\"]\n",
    "    df = pd.DataFrame(anno)\n",
    "    return df\n",
    "\n",
    "def cleanForJson(category=None, df=None):\n",
    "    # clean category for json dump\n",
    "    if category is not None:\n",
    "        category = category.rename(columns={'category_id': 'id'})\n",
    "        category['supercategory'] = \"\"\n",
    "\n",
    "    # add columns in df for json dump\n",
    "    if df is not None:\n",
    "        df['iscrowd'] = 0\n",
    "        df['attributes'] = [{'occluded':False}] * len(df['id'])\n",
    "        cols = ['id', 'image_id', 'category_id', 'segmentation', 'area', 'bbox', 'iscrowd', 'attributes']\n",
    "        df = df[cols + [c for c in df.columns if c not in cols]]\n",
    "    \n",
    "    return category, df\n",
    "\n",
    "def sort_reid(dataframe, target_column, reindex_column):\n",
    "    df2 = dataframe.sort_values(by=[target_column], ascending=True)     # create a new df with the target column sorted in ascending order\n",
    "    df2 = df2.reset_index(drop=True)                                    # we don't need the added column of old index\n",
    "    df2[reindex_column] = df2.index + 1\n",
    "    return df2\n",
    "\n",
    "# convert all np.integer, np.floating and np.ndarray into json recognisable int, float and lists\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a51d1d6-811f-4b2e-8e2e-ef4a736c85e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# filename = './input/train.json'\n",
    "filename = './input/a14-p_val.json'\n",
    "# filename = './input/Corr/val_lim4.json'\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "    if \"categories\" in data:\n",
    "        category = findCategory(data)\n",
    "    \n",
    "    if \"images\" in data:\n",
    "        images = findImages(data)\n",
    "        nos_image = images['id'].nunique()\n",
    "\n",
    "    df = findAnnotations(data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc7d845b",
   "metadata": {},
   "source": [
    "## 1a) Fix annotations inferred from Detectron2\n",
    "\n",
    "Coco_instances_results.json from COCOEvaluator only contains annotations. We need to decode the masks which are encoded in RLE for memory efficiency.\n",
    "\n",
    "Besides, we also supplement the categories from train.json and the list of images from the test/val folder (***remember to change the directory***).\n",
    "\n",
    "The final window outputs a json that combines the categories, the list of images and annotations. Feel free to use it if converting outputs from Detectron2 to CVAT-readable outputs is the main purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5fabb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pycocotools.mask as mask_util\n",
    "\n",
    "# Convert RLE counts into segmentation masks\n",
    "def polygonFromMask(maskedArr): # https://github.com/hazirbas/coco-json-converter/blob/master/generate_coco_json.py\n",
    "\n",
    "    contours, _ = cv2.findContours(maskedArr, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    segmentation = []\n",
    "    for contour in contours:\n",
    "        # Valid polygons have >= 6 coordinates (3 points)\n",
    "        if contour.size >= 6:\n",
    "            segmentation.append(contour.flatten().tolist())\n",
    "    RLEs = mask_util.frPyObjects(segmentation, maskedArr.shape[0], maskedArr.shape[1])\n",
    "    RLE = mask_util.merge(RLEs)\n",
    "    # RLE = mask.encode(np.asfortranarray(maskedArr))\n",
    "    area = mask_util.area(RLE)\n",
    "    [x, y, w, h] = cv2.boundingRect(maskedArr)\n",
    "\n",
    "    return segmentation[0], area #, [x, y, w, h], area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce358d52-4850-488e-bf91-54e40e557acc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2903/1252471046.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['segmentation'][i] = seg_concat\n",
      "/tmp/ipykernel_2903/1252471046.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['area'][i] = sample_polygon[1]\n",
      "/tmp/ipykernel_2903/1252471046.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['bbox'][i] = [round(j,2) for j in df['bbox'][i]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "\n",
    "# df = pd.DataFrame()\n",
    "filename = './input/coco_instances_results.json'\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "# add annotation id, iscrowd and attributes\n",
    "df['id'] = df.index + 1\n",
    "df['iscrowd'] = 0\n",
    "df.loc[:, 'attributes'] = [{\"occluded\":False}] * len(df['id'])\n",
    "df.insert(1, \"area\", \"\")\n",
    "\n",
    "# decode Detectron masks in RLE format\n",
    "for i in range(len(df['segmentation'])):\n",
    "    sample_mask = mask_util.decode(df['segmentation'][i])\n",
    "    sample_polygon = polygonFromMask(sample_mask)\n",
    "    seg_concat = [[]]\n",
    "    seg_concat[0][:] = sample_polygon[0]\n",
    "    df['segmentation'][i] = seg_concat\n",
    "    # Find area\n",
    "    # coord = np.transpose(np.array([seg_concat[0][::2],seg_concat[0][1::2]])) # Polygon work with dimensions (n, 2)\n",
    "    df['area'][i] = sample_polygon[1]\n",
    "\n",
    "# sort bbox to 2 decimal places\n",
    "for i in range(len(df['bbox'])):\n",
    "    df['bbox'][i] = [round(j,2) for j in df['bbox'][i]]\n",
    "\n",
    "columnsTitles = ['id', 'image_id', 'category_id', 'segmentation', 'area', 'bbox', 'iscrowd', 'attributes', 'score']\n",
    "df = df.reindex(columns=columnsTitles)\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42388f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "from PIL import Image\n",
    "\n",
    "CAT_PATH = '../images/val/val.json'\n",
    "IMAGE_CP_PATH = '../images/test/SS4/SS4_test.json'\n",
    "\n",
    "with open(CAT_PATH, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    category = findCategory(data)\n",
    "# clean category for json dump\n",
    "category = category.rename(columns={'category_id': 'id'})\n",
    "category['supercategory'] = \"\"\n",
    "\n",
    "# copy image metadata from val.json. This preserves the image_id used when inferring pseudolabels for roads_val, which uses metadata from ./val.json\n",
    "with open(IMAGE_CP_PATH, 'r') as file2:\n",
    "    data2 = json.load(file2)\n",
    "    images = findImages(data2)\n",
    "\n",
    "# or you can create the DataFrame \"images\" from scratch...\n",
    "cp_new = False\n",
    "if cp_new == True:\n",
    "    IMAGE_DIR = '../images/val/'\n",
    "    IMAGE_PATHS = glob.glob(os.path.join(IMAGE_DIR, '*.jp*g'))\n",
    "\n",
    "    for image_id, image_path in enumerate(IMAGE_PATHS):\n",
    "        img = Image.open(image_path)\n",
    "        if image_id == 0:\n",
    "            images = pd.DataFrame(columns=['id','width','height','file_name'])\n",
    "\n",
    "        add = dict()\n",
    "        add['id'] = image_id + 1\n",
    "        add['width'], add['height'] = img.size\n",
    "        add['file_name']= os.path.basename(img.filename)\n",
    "        # add['file_name'] = re.findall(r'^.*\\\\([^\\/]*)$',img.filename)[-1]\n",
    "        add_im = pd.DataFrame(add, index=[0])\n",
    "        images = pd.concat([images, add_im], ignore_index = True)   \n",
    "\n",
    "nos_image = images['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump a combined json\n",
    "\n",
    "# Write categories, images and annotations as arrays containing individual entries as a dictionary\n",
    "# see https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_dict.html for to_dict styles\n",
    "dict_to_json = {\n",
    "    \"categories\": category.to_dict('records'),\n",
    "    \"images\": images.to_dict('records'),\n",
    "    \"annotations\": df.to_dict('records')\n",
    "    }\n",
    "\n",
    "with open(\"./SS4_inferred.json\", \"w\") as outfile:\n",
    "    json.dump(dict_to_json, outfile, cls=NpEncoder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb375ecd-bab2-47a1-a000-a5f5f500ad14",
   "metadata": {},
   "source": [
    "# 3) Convert COCO JSON to YOLO Labels File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0be6f26e-002d-476d-aca6-73f240021e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10317/1271686149.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  category_want.rename(columns={'category_id': 'category_id_old'}, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels of 557 images prepared!\n",
      "custom.yaml prepared!\n"
     ]
    }
   ],
   "source": [
    "import re, os\n",
    "import yaml\n",
    "\n",
    "# For training without specific categories\n",
    "cat_elim = [1, 2, 5, 6, 7, 8, 9, 11, 12, 13]\n",
    "out_dir = './labels'\n",
    "rearr_cat = True\n",
    "\n",
    "# Create the df of categories I want\n",
    "df_want = df[~df['category_id'].isin(cat_elim)]\n",
    "category_want = category[~category['category_id'].isin(cat_elim)]\n",
    "# rearrange the category_id to start from 1\n",
    "if rearr_cat == True:\n",
    "    category_want.rename(columns={'category_id': 'category_id_old'}, inplace=True)\n",
    "    category_want = sort_reid(category_want, 'category_id_old', 'category_id')\n",
    "    df_want = df_want.merge(category_want[['category_id','category_id_old']], left_on='category_id', right_on='category_id_old')\n",
    "    df_want = df_want.drop(columns=['category_id_old','category_id_x'])\n",
    "    df_want = df_want.rename(columns={'category_id_y': 'category_id'})\n",
    "    category_want = category_want.drop(columns=['category_id_old'])\n",
    "# images_want = images[images['id'].isin(df_want['image_id'])]            # delete in images the images that are not in df_want\n",
    "images = images\n",
    "category = category_want\n",
    "df = sort_reid(df_want, 'image_id', 'id')\n",
    "del df_want, category_want\n",
    "\n",
    "# find if image_id has any defects\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "for i in images['id']:\n",
    "    if (i in df['image_id'].unique()) == True:\n",
    "        # print(f'image {i} has defect')\n",
    "        \n",
    "        # Concatenate df and images for the text file\n",
    "        df2 = df[df['image_id']==i]\n",
    "        image2 = images.loc[(images['id']==i),['id','width','height','file_name']]\n",
    "        image2 = image2.rename(columns={'id': 'image_id'})\n",
    "        df2 = df2.drop(columns=['area','bbox','iscrowd','attributes'])\n",
    "        df2 = df2.merge(image2,left_on='image_id',right_on='image_id') \n",
    "        \n",
    "        # Normalise polygon points\n",
    "        poly_print = []\n",
    "        im_width = df2['width'][0]\n",
    "        im_height = df2['height'][0]\n",
    "        for f, x in enumerate(df2['segmentation'].to_list()):\n",
    "            # if df2['category_id'][f] in cat_elim:\n",
    "            #     # skip unwanted categories\n",
    "            #     continue\n",
    "            # else:\n",
    "            poly = [df2['category_id'][f]-1]\n",
    "            # print(f'image {i} polygon length {len(x[0])}')\n",
    "            for y in range(1,len(x[0]),2):\n",
    "                # print(x,y)\n",
    "                ptx = x[0][y-1]/im_width\n",
    "                pty = x[0][y]/im_height\n",
    "                poly.extend([round(min(ptx,1),4), round(min(pty, 1),4)])              # set mins to 1 to prevent any out-of-bounds conversions\n",
    "            # poly.extend([poly[1],poly[2]])                      # YOLO does not need polygon points to form a close loop\n",
    "            poly_print.append(poly)\n",
    "            poly = []\n",
    "        \n",
    "        # fname = re.findall(r'(.*)(?:.jpg)$',df2['file_name'][0])[0]\n",
    "        fname = os.path.splitext(df2['file_name'][0])[0]\n",
    "        fname = os.path.join(out_dir, f'{fname}.txt')\n",
    "        with open(fname, \"w\") as outfile:\n",
    "            for num, j in enumerate(poly_print):\n",
    "                outfile.write(' '.join(str(e) for e in j))\n",
    "                # print(f'image {i} defect {num} of {len(poly_print)}')\n",
    "                if num==len(poly_print)-1: outfile.write('')\n",
    "                else: outfile.write('\\n')\n",
    "        poly_print = []\n",
    "    else:\n",
    "        # fname = re.findall(r'(.*)(?:.jpg)$',images.loc[(images['id']==i),'file_name'].values[0])[0]\n",
    "        fname = os.path.splitext(images.loc[(images['id']==i),'file_name'].values[0])[0]\n",
    "        fname = os.path.join(out_dir, f'{fname}.txt')\n",
    "        with open(fname, \"w\") as outfile:\n",
    "            outfile.write('')\n",
    "\n",
    "print(f'Labels of {nos_image} images prepared!')\n",
    "\n",
    "# Create data.yaml file\n",
    "nc = len(category['category_id'])\n",
    "names = category['name'].to_list()\n",
    "with open(r'./custom.yaml', \"w\") as outfile:\n",
    "    outfile.write('train: a14-p/train/images \\n')\n",
    "    outfile.write('val: a14-p/val/images \\n')\n",
    "    outfile.write('test: a14-p/test/images \\n \\n')\n",
    "    outfile.write(f'nc: {nc} \\n')\n",
    "    outfile.write(f'names: {names} \\n')\n",
    "print('custom.yaml prepared!')\n",
    "\n",
    "# np.array(df2['segmentation'].to_list()[0])/10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f0a942a-6633-43c6-b273-8738d8f44f5e",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Now move the folder \"txt_labels\" to the corresponding ./data/[test/train/val] folder\n",
    "\n",
    "Move also a the custom.yaml to ./data\n",
    "\n",
    "Leave the folder \"txt_labels\" in this ./fix_annotation folder for subsequent annotation files.\n",
    "\n",
    "Happy training!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f371cfe3",
   "metadata": {},
   "source": [
    "# 4) Convert YOLO Labels to COCO JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97e3dfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defect id: 139, image: ['A12Area6SwallowsCrossMountnessingHuttonBrentwoodLondon_sideview_000020_003239_Lane2.jpg']\n",
      "defect id: 148, image: ['A12Area6SwallowsCrossMountnessingHuttonBrentwoodLondon_sideview_000020_003239_Lane2.jpg']\n",
      "defect id: 162, image: ['A12Area6SwallowsCrossMountnessingHuttonBrentwoodLondon_sideview_000020_003242_Lane2.jpg']\n",
      "defect id: 187, image: ['A12Area6SwallowsCrossMountnessingHuttonBrentwoodLondon_sideview_000020_003357_Lane2.jpg']\n",
      "defect id: 226, image: ['A12Area6SwallowsCrossMountnessingHuttonBrentwoodLondon_sideview_000023_000238_Lane1.jpg']\n",
      "defect id: 277, image: ['A12Area6SwallowsCrossMountnessingHuttonBrentwoodLondon_sideview_000023_000345_Lane1.jpg']\n",
      "defect id: 307, image: ['A12Area6SwallowsCrossMountnessingHuttonBrentwoodLondon_sideview_000023_000392_Lane1.jpg']\n",
      "defect id: 387, image: ['A12Area6SwallowsCrossMountnessingHuttonBrentwoodLondon_sideview_000023_000671_Lane1.jpg']\n",
      "defect id: 564, image: ['A12Area6SwallowsCrossMountnessingHuttonBrentwoodLondon_sideview_000023_001012_Lane1.jpg']\n",
      "defect id: 660, image: ['A12Area6SwallowsCrossMountnessingHuttonBrentwoodLondon_sideview_000024_000507_Lane1.jpg']\n",
      "defect id: 720, image: ['A12Area6SwallowsCrossMountnessingHuttonBrentwoodLondon_sideview_000025_001361_Lane2.jpg']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>segmentation</th>\n",
       "      <th>area</th>\n",
       "      <th>bbox</th>\n",
       "      <th>iscrowd</th>\n",
       "      <th>attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>[[2410.0, 638.8, 2406.3, 642.5, 2406.3, 653.8,...</td>\n",
       "      <td>3226.7</td>\n",
       "      <td>[2402.4, 638.8, 57.7, 64.1]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'occluded': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>[[2329.2, 393.1, 2325.3, 396.8, 2302.4, 396.8,...</td>\n",
       "      <td>17230.7</td>\n",
       "      <td>[2294.7, 393.1, 165.4, 147.4]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'occluded': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>[[223.2, 362.9, 219.5, 366.6, 188.7, 366.6, 18...</td>\n",
       "      <td>11394.8</td>\n",
       "      <td>[142.4, 362.9, 146.4, 98.3]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'occluded': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>[[950.9, 75.7, 947.2, 79.4, 931.6, 79.4, 927.9...</td>\n",
       "      <td>12392.9</td>\n",
       "      <td>[924.0, 75.7, 219.5, 64.1]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'occluded': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>[[308.0, 211.6, 304.1, 215.5, 296.4, 215.5, 28...</td>\n",
       "      <td>17849.3</td>\n",
       "      <td>[246.4, 211.6, 219.5, 98.2]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'occluded': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>726</td>\n",
       "      <td>850</td>\n",
       "      <td>11</td>\n",
       "      <td>[[169.5, 302.4, 165.6, 306.1, 138.7, 306.1, 13...</td>\n",
       "      <td>29376.9</td>\n",
       "      <td>[76.9, 302.4, 396.7, 192.7]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'occluded': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>727</td>\n",
       "      <td>853</td>\n",
       "      <td>11</td>\n",
       "      <td>[[365.7, 166.3, 362.0, 170.0, 288.8, 170.0, 28...</td>\n",
       "      <td>25299.0</td>\n",
       "      <td>[215.6, 166.3, 358.0, 162.5]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'occluded': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>728</td>\n",
       "      <td>854</td>\n",
       "      <td>11</td>\n",
       "      <td>[[111.6, 393.1, 107.7, 396.8, 104.0, 396.8, 10...</td>\n",
       "      <td>57148.7</td>\n",
       "      <td>[0.0, 393.1, 512.0, 222.9]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'occluded': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>729</td>\n",
       "      <td>855</td>\n",
       "      <td>11</td>\n",
       "      <td>[[585.2, 151.1, 581.3, 155.0, 362.0, 155.0, 35...</td>\n",
       "      <td>22138.1</td>\n",
       "      <td>[261.7, 151.1, 389.0, 128.5]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'occluded': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>730</td>\n",
       "      <td>857</td>\n",
       "      <td>11</td>\n",
       "      <td>[[400.4, 136.1, 396.5, 139.8, 381.2, 139.8, 37...</td>\n",
       "      <td>26066.8</td>\n",
       "      <td>[296.4, 136.1, 358.0, 143.5]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'occluded': False}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>719 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id image_id category_id  \\\n",
       "0      1        1          11   \n",
       "1      2        2          11   \n",
       "2      3        2          11   \n",
       "3      4        4          11   \n",
       "4      5        5          11   \n",
       "..   ...      ...         ...   \n",
       "714  726      850          11   \n",
       "715  727      853          11   \n",
       "716  728      854          11   \n",
       "717  729      855          11   \n",
       "718  730      857          11   \n",
       "\n",
       "                                          segmentation     area  \\\n",
       "0    [[2410.0, 638.8, 2406.3, 642.5, 2406.3, 653.8,...   3226.7   \n",
       "1    [[2329.2, 393.1, 2325.3, 396.8, 2302.4, 396.8,...  17230.7   \n",
       "2    [[223.2, 362.9, 219.5, 366.6, 188.7, 366.6, 18...  11394.8   \n",
       "3    [[950.9, 75.7, 947.2, 79.4, 931.6, 79.4, 927.9...  12392.9   \n",
       "4    [[308.0, 211.6, 304.1, 215.5, 296.4, 215.5, 28...  17849.3   \n",
       "..                                                 ...      ...   \n",
       "714  [[169.5, 302.4, 165.6, 306.1, 138.7, 306.1, 13...  29376.9   \n",
       "715  [[365.7, 166.3, 362.0, 170.0, 288.8, 170.0, 28...  25299.0   \n",
       "716  [[111.6, 393.1, 107.7, 396.8, 104.0, 396.8, 10...  57148.7   \n",
       "717  [[585.2, 151.1, 581.3, 155.0, 362.0, 155.0, 35...  22138.1   \n",
       "718  [[400.4, 136.1, 396.5, 139.8, 381.2, 139.8, 37...  26066.8   \n",
       "\n",
       "                              bbox iscrowd           attributes  \n",
       "0      [2402.4, 638.8, 57.7, 64.1]       0  {'occluded': False}  \n",
       "1    [2294.7, 393.1, 165.4, 147.4]       0  {'occluded': False}  \n",
       "2      [142.4, 362.9, 146.4, 98.3]       0  {'occluded': False}  \n",
       "3       [924.0, 75.7, 219.5, 64.1]       0  {'occluded': False}  \n",
       "4      [246.4, 211.6, 219.5, 98.2]       0  {'occluded': False}  \n",
       "..                             ...     ...                  ...  \n",
       "714    [76.9, 302.4, 396.7, 192.7]       0  {'occluded': False}  \n",
       "715   [215.6, 166.3, 358.0, 162.5]       0  {'occluded': False}  \n",
       "716     [0.0, 393.1, 512.0, 222.9]       0  {'occluded': False}  \n",
       "717   [261.7, 151.1, 389.0, 128.5]       0  {'occluded': False}  \n",
       "718   [296.4, 136.1, 358.0, 143.5]       0  {'occluded': False}  \n",
       "\n",
       "[719 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob, os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from PIL import Image\n",
    "# from imantics import Mask\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "IMAGE_DIR = '../data/test/images/'\n",
    "# IMAGE_DIR = os.path.join(os.getcwd(), '..' ) + '/data/test/images/'\n",
    "IMAGE_PATHS = glob.glob(os.path.join(IMAGE_DIR, '*.jp*g'))\n",
    "LABEL_DIR = './input/'\n",
    "LABEL_PATHS = glob.glob(os.path.join(LABEL_DIR, '*.txt'))\n",
    "# SAVE_DIR = './labels/test_annotated/'\n",
    "\n",
    "with open(\"custom.yaml\", \"r\") as cat:\n",
    "    data = yaml.safe_load(cat)\n",
    "    category = pd.DataFrame(data['names'], columns=['name'])\n",
    "    category['id'] = category.index + 1\n",
    "    category['supercategory'] = \"\"\n",
    "    category = category[['id','name','supercategory']]\n",
    "\n",
    "\n",
    "for image_id, image_path in enumerate(IMAGE_PATHS):\n",
    "    img = Image.open(image_path)\n",
    "    if image_id == 0:\n",
    "        image = pd.DataFrame(columns=['id','width','height','file_name'])\n",
    "\n",
    "    add = dict()\n",
    "    add['id'] = image_id + 1\n",
    "    add['width'], add['height'] = img.size\n",
    "    # add['file_name']= img.filename.split(\"/\")[-1]\n",
    "    add['file_name'] = re.findall(r'^.*\\\\([^\\/]*)$',img.filename)[-1]\n",
    "    add_im = pd.DataFrame(add, index=[0])\n",
    "    image = pd.concat([image, add_im], ignore_index = True)   \n",
    "\n",
    "\n",
    "\n",
    "for txt_id, txt_path in enumerate(LABEL_PATHS):\n",
    "    if txt_id == 0:\n",
    "        df = pd.DataFrame(columns=['id','image_id','category_id','segmentation','area','bbox','iscrowd', 'attributes'])\n",
    "        full_anno = pd.DataFrame()\n",
    "    \n",
    "    with open(txt_path, 'r', encoding='utf-8-sig', errors=\"surrogateescape\") as file:\n",
    "        # if df.shape[0]==0:\n",
    "        if len(df['id']) == 0:\n",
    "            prev_defect = 0\n",
    "        else:\n",
    "            prev_defect = max(df['id'])\n",
    "        \n",
    "        for i, line in enumerate(file):\n",
    "            inner_list = [float(elt.strip()) for elt in line.split(' ')]\n",
    "            defect = dict()\n",
    "            \n",
    "            filename = re.findall(r'^.*\\\\([^\\/]*)(?:s|_mask.txt)$',txt_path)[-1] + \".jpg\"\n",
    "            # filename = re.findall(r'^.*\\\\([^\\/]*)(?:s|.txt)$',txt_path)[-1] + \".jpg\"\n",
    "            defect['id'] = prev_defect + i + 1\n",
    "            defect['image_id'] = image.loc[(image['file_name']==filename), 'id'].values[0]\n",
    "            # defect['image_id'] = txt_id + 1\n",
    "            defect['category_id'] = int(inner_list[0]) + 1\n",
    "            \n",
    "            # upscale segmentation\n",
    "            seg = np.array(inner_list[1:])\n",
    "            x_scale = image.loc[(image['id']==defect['image_id']), 'width'].values      # x width\n",
    "            y_scale = image.loc[(image['id']==defect['image_id']), 'height'].values     # y height\n",
    "            # print(x_scale, y_scale)\n",
    "            for y in range(1,len(seg),2):\n",
    "                seg[y] = round(float(seg[y] * y_scale),1)\n",
    "                seg[y-1] = round(float(seg[y-1] * x_scale),1)    \n",
    "            seg_concat = [[]]\n",
    "            seg_concat[0][:] = seg.tolist()\n",
    "            defect['segmentation'] = seg_concat\n",
    "            # defect['segmentation'] = seg.to_list()\n",
    "\n",
    "            # Find bbox\n",
    "            x_min, x_max = min(seg[::2]), max(seg[::2])\n",
    "            y_min, y_max = min(seg[1::2]), max(seg[1::2])\n",
    "            defect['bbox'] = [x_min, y_min, round(x_max-x_min,1), round(y_max-y_min,1)]\n",
    "            # Find area\n",
    "            coord = np.transpose(np.array([seg[::2],seg[1::2]])) # Polygon work with dimensions (n, 2)\n",
    "            try:\n",
    "                defect['area'] = round(Polygon(coord).area,1)\n",
    "            except:\n",
    "                print('defect id: {}, image: {}'.format(defect['id'], image.loc[(image['id']==defect['image_id']), 'file_name'].values))\n",
    "                continue\n",
    "            defect['iscrowd'] = 0\n",
    "            defect['attributes'] = {\"occluded\":False}\n",
    "\n",
    "            add_df = pd.DataFrame([defect], index=[0])\n",
    "            df = pd.concat([df, add_df], ignore_index = True)\n",
    "\n",
    "\n",
    "# Write categories, images and annotations as arrays containing individual entries as a dictionary\n",
    "# see https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_dict.html for to_dict styles\n",
    "dict_to_json = {\n",
    "    \"categories\": category.to_dict('records'),\n",
    "    \"images\": image.to_dict('records'),\n",
    "    \"annotations\": df.to_dict('records')\n",
    "    }\n",
    "\n",
    "# convert all np.integer, np.floating and np.ndarray into json recognisable int, float and lists\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "with open(\"./inferred.json\", \"w\") as outfile:\n",
    "    json.dump(dict_to_json, outfile, cls=NpEncoder)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cee878fe",
   "metadata": {},
   "source": [
    "# 5) Combine COCO JSON\n",
    "\n",
    "Useful when patches and crack_longitudinal are detected using one trained model, while other defects are detected with another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to fiddle around with the json file\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def createDF(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "        category = findCategory(data)\n",
    "        images = findImages(data)\n",
    "        nos_image = images['id'].max()\n",
    "        df = findAnnotations(data)\n",
    "        df = df.merge(images[['id','file_name']], left_on='image_id', right_on='id')\n",
    "        df = df.rename(columns={'id_x': 'id'})\n",
    "        df = drop_columns_if_exist(df,columns=['iscrowd','attributes','id_y'])\n",
    "        return category, images, df\n",
    "\n",
    "def drop_columns_if_exist(df, columns):\n",
    "    df = df.copy()\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(columns=col)\n",
    "    return df\n",
    "\n",
    "# if categories aren't supposed to be the same\n",
    "# category 1 and df are the ones you want to change\n",
    "def fix_category_id(category1, category2, df, df2):\n",
    "    \n",
    "    category_name = pd.concat([category1['name'], category2['name']], ignore_index = True)      # combine category 1 and 2\n",
    "    unique_category_names = category_name.unique()                                              # Get unique category names\n",
    "    # Create a new DataFrame with these unique category names and a new column for the relisted category_id\n",
    "    category_new = pd.DataFrame({\n",
    "        'category_id': range(1, len(unique_category_names) + 1),\n",
    "        'name': unique_category_names\n",
    "    })\n",
    "\n",
    "    category1 = category1.merge(category_new, on='name', how='left')        # map category_id to category1 and category 2\n",
    "    category2 = category2.merge(category_new, on='name', how='left')        # map category_id to category1 and category 2\n",
    "\n",
    "    # use new category_id to replace old category_id in df\n",
    "    df = df.merge(category1[['category_id_x', 'category_id_y']], left_on='category_id', right_on='category_id_x', how='left')\n",
    "    df2 = df2.merge(category2[['category_id_x', 'category_id_y']], left_on='category_id', right_on='category_id_x', how='left')\n",
    "    df = df.drop(columns=['category_id', 'category_id_x']).rename(columns={'category_id_y': 'category_id'})\n",
    "    df2 = df2.drop(columns=['category_id', 'category_id_x']).rename(columns={'category_id_y': 'category_id'})\n",
    "\n",
    "    return category_new, df, df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fbb4e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to combine segmentation masks\n",
    "from pycocotools import mask as cocomask\n",
    "import cv2\n",
    "\n",
    "def polygon_to_mask(polygons, height, width):\n",
    "    '''\n",
    "    Args:\n",
    "        polygon (list): in [[...]]\n",
    "        height, width (int): height and width of image\n",
    "    Returns:\n",
    "        np.array: a mask of shape (H, W)\n",
    "    '''\n",
    "    the_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    # tackle the problem of having >1 polygon in an instance\n",
    "    if len(polygons) > 1:\n",
    "        for poly in polygons:\n",
    "            # poly = np.array(poly).reshape((-1, 2))\n",
    "            rles = cocomask.frPyObjects([poly], height, width)\n",
    "            a_mask = np.squeeze(cocomask.decode(rles))\n",
    "            print(a_mask.shape)\n",
    "            the_mask = np.logical_or(the_mask, a_mask)\n",
    "    else:\n",
    "        rles = cocomask.frPyObjects(polygons, height, width)\n",
    "        the_mask = np.squeeze(cocomask.decode(rles))\n",
    "\n",
    "    return the_mask\n",
    "\n",
    "def mask_iou(masks):\n",
    "    \"\"\"\n",
    "    Calculate the IoU of a list of masks.\n",
    "    Args:\n",
    "        masks (list): a list of masks of shape (N, H, W).\n",
    "    Returns:\n",
    "        np.array: outputs of a matrix of shape (len(masks), len(masks)).\n",
    "    \"\"\"\n",
    "    num_masks = len(masks)\n",
    "    mask_ious = np.zeros((num_masks, num_masks), dtype=np.float32)\n",
    "\n",
    "    # Calculate the intersection and union of masks using logical AND and OR operation\n",
    "    for i in range(num_masks):\n",
    "        for j in range(i+1, num_masks):\n",
    "            intersection = np.logical_and(masks[i], masks[j])\n",
    "            union = np.logical_or(masks[i], masks[j])\n",
    "            iou = np.sum(intersection) / np.sum(union)\n",
    "            mask_ious[i, j] = iou\n",
    "            mask_ious[j, i] = iou\n",
    "\n",
    "    return mask_ious\n",
    "\n",
    "def mask_to_polygon(maskedArr):\n",
    "    \"\"\"\n",
    "    Convert a mask in a binary matrix of shape (H, W) to a polygon.\n",
    "    Args:\n",
    "        maskedArr (np.array): a binary matrix of shape (H,W) that contains True/False value\n",
    "    Returns:\n",
    "        segmentation[0] (np.array): an array of the polygon. [x1, y1, x2, y2...]\n",
    "        area (float): the size of the polygon\n",
    "        bbox (list): Bounding box coordinates in COCO format [x_min, y_min, delta_x, delta_y].\n",
    "    \"\"\"\n",
    "    # Convert the binary mask to a binary image (0 and 255 values)\n",
    "    binary_image = maskedArr.astype(np.uint8) * 255\n",
    "\n",
    "    # Find contours in the binary image\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Retrieve the polygon coordinates for each contour\n",
    "    polygons = []\n",
    "    for contour in contours:\n",
    "        if contour.size >= 6:\n",
    "            polygons.append(contour.flatten().tolist())\n",
    "        # polygon = contour.squeeze().tolist()\n",
    "        # polygons.append(polygon)\n",
    "    # Calculate areas and bbox\n",
    "    area = np.sum(maskedArr)    \n",
    "    # RLEs = mask_util.frPyObjects(segmentation, maskedArr.shape[0], maskedArr.shape[1])\n",
    "    # RLE = mask_util.merge(RLEs)\n",
    "    # area = mask_util.area(RLE)\n",
    "    bbox = find_remasked_bbox(maskedArr)\n",
    "\n",
    "    return polygons, area, bbox\n",
    "\n",
    "def merge_masks(mask1, mask2):\n",
    "    \"\"\"\n",
    "    Merge two masks.\n",
    "    Args:\n",
    "        mask1 (np.array): First mask of shape (H, W).\n",
    "        mask2 (np.array): Second mask of shape (H, W).\n",
    "    Returns:\n",
    "        np.array: Merged mask if IoU is above the threshold, otherwise None.\n",
    "    \"\"\"\n",
    "    merged_mask = np.logical_or(mask1, mask2)\n",
    "    return merged_mask\n",
    "\n",
    "# also need to fix areas\n",
    "def find_remasked_bbox(mask):\n",
    "    \"\"\"\n",
    "    Find the bounding box that fits the resized mask.\n",
    "\n",
    "    Args:\n",
    "        mask (np.array): Resized mask of shape (H', W').\n",
    "        original_shape (tuple): Shape of the original image or mask (H, W).\n",
    "\n",
    "    Returns:\n",
    "        list: Bounding box coordinates in COCO format [x_min, y_min, delta_x, delta_y].\n",
    "    \"\"\"\n",
    "    # Find the minimum and maximum coordinates that enclose the mask region\n",
    "    rows = np.any(mask, axis=1)\n",
    "    cols = np.any(mask, axis=0)\n",
    "    y_min, y_max = np.where(rows)[0][[0, -1]]\n",
    "    x_min, x_max = np.where(cols)[0][[0, -1]]\n",
    "\n",
    "    return [x_min, y_min, x_max-x_min, y_max-y_min]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6cd4b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2056, 2464)\n",
      "(2056, 2464)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Files to be combined\n",
    "filename = '../data/A12AL/A12AL_train4_00.json'\n",
    "filename2 = '../data/A12AL/A12AL_rest4.json'\n",
    "\n",
    "# Inputs\n",
    "iou_merge_thres = 0.5          # IoU for merging polygons\n",
    "same_category = True          # True if categories are supposed to be the same\n",
    "\n",
    "# Store categories, images and annotations in separate dataframes\n",
    "category, images, df = createDF(filename)\n",
    "category2, images2, df2 = createDF(filename2)\n",
    "\n",
    "# Categories\n",
    "if same_category == True:\n",
    "    # if categories are supposed to be the same \n",
    "    # check length\n",
    "    if len(category) != len(category2):\n",
    "        print('categories not the same. Check before proceeding')\n",
    "    # check each category\n",
    "    for i in range(len(category['name'])):\n",
    "        if category['name'][i] != category2['name'][i]:\n",
    "            print('category id: {} , {} in file 1 different from category id: {} , {} in file 2. Please check'.format(category['category_id'][i], category['name'][i], category2['category_id'][i], category2['name'][i]))\n",
    "    category_new = category\n",
    "else:\n",
    "    category_new, df, df2 = fix_category_id(category, category2, df, df2)\n",
    "\n",
    "category_new, _ = cleanForJson(category=category_new)\n",
    "\n",
    "# Annotations\n",
    "# merge two annotation df\n",
    "df_new = pd.concat([df, df2], ignore_index = True)\n",
    "\n",
    "# combine and re-arrange the image info\n",
    "images_new = pd.DataFrame(columns=['id','width','height','file_name'])\n",
    "images_new['file_name'] = df_new['file_name'].unique()\n",
    "images_new = images_new.sort_values(by=['file_name'], ignore_index=True)\n",
    "images_new['id'] = images_new.index + 1\n",
    "for i in range(len(images_new['file_name'])):\n",
    "    # find out which json records the concerned image\n",
    "    if images_new['file_name'][i] in images['file_name'].tolist():\n",
    "        dim = images.loc[(images['file_name']==images_new['file_name'][i]),['width','height']]\n",
    "    elif images_new['file_name'][i] in images2['file_name'].tolist():\n",
    "        dim = images2.loc[(images2['file_name']==images_new['file_name'][i]),['width','height']]\n",
    "    else:\n",
    "        print('image not included')\n",
    "        continue\n",
    "    # paste width and height info\n",
    "    images_new.loc[i,'width'] = dim['width'].values[0]\n",
    "    images_new.loc[i,'height'] = dim['height'].values[0]\n",
    "\n",
    "\n",
    "# assign new image id and defect id in df_new\n",
    "for i in range(len(df_new['id'])):\n",
    "    df_new.loc[i, 'image_id'] = images_new.loc[(images_new['file_name']==df_new['file_name'][i]), 'id'].values\n",
    "df_new = df_new.sort_values(by=['image_id'], ignore_index=True)\n",
    "df_new['id'] = df_new.index + 1\n",
    "_, df_new = cleanForJson(df=df_new)\n",
    "df_new = df_new.drop(columns=['file_name'])\n",
    "\n",
    "# combine masks in the same image\n",
    "for i, v in enumerate(images_new['id']):\n",
    "    height_i = images_new.loc[i,'height']\n",
    "    width_i = images_new.loc[i,'width']\n",
    "    df_check = df_new[df_new['image_id']== v]\n",
    "    df_check.reset_index(drop=True)\n",
    "    seg_polygons = df_check['segmentation'].to_list()\n",
    "    seg_catid = df_check['category_id'].to_list()\n",
    "\n",
    "    # convert polygons of an image to masks\n",
    "    seg_masks = []\n",
    "    for idplygn, x in enumerate(seg_polygons):\n",
    "        mask = polygon_to_mask(x, height_i, width_i)\n",
    "        seg_masks.append(mask)\n",
    "    \n",
    "    # calculate iou of all instances\n",
    "    masks_iou = mask_iou(seg_masks)\n",
    "    \n",
    "    # merge masks\n",
    "    for i in range(masks_iou.shape[0]):\n",
    "        for j in range(i + 1, masks_iou.shape[1]):\n",
    "            the_mask_iou = masks_iou[i, j]\n",
    "            if the_mask_iou > iou_merge_thres and seg_catid[i]==seg_catid[j]:\n",
    "                merged_mask = merge_masks(seg_masks[i],seg_masks[j])\n",
    "                merged_polygon = mask_to_polygon(merged_mask)\n",
    "                seg_concat = []\n",
    "                seg_concat[:] = merged_polygon[0]\n",
    "                # add the new merged polygon to the i-th entry\n",
    "                df_check.at[i, 'segmentation'] = seg_concat\n",
    "                df_check.at[i,'area'] = merged_polygon[1]                     # new area for the new mask\n",
    "                df_check.at[i,'bbox'] = find_remasked_bbox(merged_mask)       # new bbox for the new mask\n",
    "                # clear j-th entry \n",
    "                df_check.at[j,'segmentation'] = []\n",
    "                df_check.at[j,'area'] = np.NaN\n",
    "                df_check.at[j,'bbox'] = []\n",
    "\n",
    "    df_check = df_check.dropna(subset=['area'])\n",
    "    df_new = df_new.drop(df_new[df_new.image_id == v].index)\n",
    "    df_new = pd.concat([df_new,df_check], ignore_index=True)\n",
    "\n",
    "columnsTitles = ['id', 'image_id', 'category_id', 'segmentation', 'area', 'bbox', 'iscrowd', 'attributes', 'score']\n",
    "df_new = df_new.reindex(columns=columnsTitles)\n",
    "df_new['id'] = df_new.index + 1\n",
    "# clear the \"score\" column if there are no scores (i.e. combining Ground Truths)\n",
    "if df_new['score'].isnull().sum() == len(df_new['score']):\n",
    "    df_new = df_new.drop(columns=['score'])\n",
    "\n",
    "# Write categories, images and annotations as arrays containing individual entries as a dictionary\n",
    "# see https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_dict.html for to_dict styles\n",
    "dict_to_json = {\n",
    "    \"categories\": category_new.to_dict('records'),\n",
    "    \"images\": images_new.to_dict('records'),\n",
    "    \"annotations\": df_new.to_dict('records')\n",
    "    }\n",
    "\n",
    "with open(\"./A12AL_train4_all.json\", \"w\") as outfile:\n",
    "    json.dump(dict_to_json, outfile, cls=NpEncoder)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e517fff3",
   "metadata": {},
   "source": [
    "# Dataframe formats for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00ea8bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1232</td>\n",
       "      <td>1028</td>\n",
       "      <td>A14EBWBJ47AWoolpittoHaugleyBridge_sideview_000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1232</td>\n",
       "      <td>1028</td>\n",
       "      <td>A14EBWBJ47AWoolpittoHaugleyBridge_sideview_000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1232</td>\n",
       "      <td>1028</td>\n",
       "      <td>A14EBWBJ47AWoolpittoHaugleyBridge_sideview_000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1232</td>\n",
       "      <td>1028</td>\n",
       "      <td>A14EBWBJ47AWoolpittoHaugleyBridge_sideview_000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1232</td>\n",
       "      <td>1028</td>\n",
       "      <td>A14EBWBJ47AWoolpittoHaugleyBridge_sideview_000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5878</th>\n",
       "      <td>5879</td>\n",
       "      <td>1232</td>\n",
       "      <td>1028</td>\n",
       "      <td>A14EBWBJ47AWoolpittoHaugleyBridge_sideview_000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5879</th>\n",
       "      <td>5880</td>\n",
       "      <td>1232</td>\n",
       "      <td>1028</td>\n",
       "      <td>A14EBWBJ47AWoolpittoHaugleyBridge_sideview_000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5880</th>\n",
       "      <td>5881</td>\n",
       "      <td>1232</td>\n",
       "      <td>1028</td>\n",
       "      <td>A14EBWBJ47AWoolpittoHaugleyBridge_sideview_000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5881</th>\n",
       "      <td>5882</td>\n",
       "      <td>1232</td>\n",
       "      <td>1028</td>\n",
       "      <td>A14EBWBJ47AWoolpittoHaugleyBridge_sideview_000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5882</th>\n",
       "      <td>5883</td>\n",
       "      <td>1232</td>\n",
       "      <td>1028</td>\n",
       "      <td>A14EBWBJ47AWoolpittoHaugleyBridge_sideview_000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5883 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  width  height                                          file_name\n",
       "0        1   1232    1028  A14EBWBJ47AWoolpittoHaugleyBridge_sideview_000...\n",
       "1        2   1232    1028  A14EBWBJ47AWoolpittoHaugleyBridge_sideview_000...\n",
       "2        3   1232    1028  A14EBWBJ47AWoolpittoHaugleyBridge_sideview_000...\n",
       "3        4   1232    1028  A14EBWBJ47AWoolpittoHaugleyBridge_sideview_000...\n",
       "4        5   1232    1028  A14EBWBJ47AWoolpittoHaugleyBridge_sideview_000...\n",
       "...    ...    ...     ...                                                ...\n",
       "5878  5879   1232    1028  A14EBWBJ47AWoolpittoHaugleyBridge_sideview_000...\n",
       "5879  5880   1232    1028  A14EBWBJ47AWoolpittoHaugleyBridge_sideview_000...\n",
       "5880  5881   1232    1028  A14EBWBJ47AWoolpittoHaugleyBridge_sideview_000...\n",
       "5881  5882   1232    1028  A14EBWBJ47AWoolpittoHaugleyBridge_sideview_000...\n",
       "5882  5883   1232    1028  A14EBWBJ47AWoolpittoHaugleyBridge_sideview_000...\n",
       "\n",
       "[5883 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d481446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>bleeding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>raveling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>crack_transverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>crack_longitudinal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>crack_edge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>crack_alligator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>crack_block</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>shoving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>rutting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>potholes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>patch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>crack_corner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>spalling</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    category_id                name\n",
       "0             1            bleeding\n",
       "1             2            raveling\n",
       "2             3    crack_transverse\n",
       "3             4  crack_longitudinal\n",
       "4             5          crack_edge\n",
       "5             6     crack_alligator\n",
       "6             7         crack_block\n",
       "7             8             shoving\n",
       "8             9             rutting\n",
       "9            10            potholes\n",
       "10           11               patch\n",
       "11           12             unknown\n",
       "12           13        crack_corner\n",
       "13           14            spalling"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b3bc233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>segmentation</th>\n",
       "      <th>area</th>\n",
       "      <th>bbox</th>\n",
       "      <th>iscrowd</th>\n",
       "      <th>attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>[[0.0, 425.2, 319.15, 403.27, 682.85, 415.64, ...</td>\n",
       "      <td>160258.0</td>\n",
       "      <td>[0.0, 341.42, 2464.0, 98.97]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'occluded': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>[[806.55, 38.64, 854.4, 34.01, 880.64, 64.88, ...</td>\n",
       "      <td>7664.0</td>\n",
       "      <td>[806.55, 34.01, 422.93, 290.19]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'occluded': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>[[84.11, 185.55, 145.96, 184.31, 174.42, 145.9...</td>\n",
       "      <td>2652.0</td>\n",
       "      <td>[84.11, 139.78, 90.31, 45.77]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'occluded': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>[[947.02, 407.55, 1002.58, 407.55, 1021.11, 44...</td>\n",
       "      <td>8810.0</td>\n",
       "      <td>[947.02, 407.55, 284.01, 321.05]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'occluded': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>[[718.57, 6.23, 754.07, 47.9, 794.2, 89.58, 82...</td>\n",
       "      <td>18234.0</td>\n",
       "      <td>[718.57, 3.14, 327.23, 395.15]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'occluded': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5417</th>\n",
       "      <td>5418</td>\n",
       "      <td>5872</td>\n",
       "      <td>12</td>\n",
       "      <td>[[1113.35, 643.26, 1459.73, 640.79, 1459.73, 5...</td>\n",
       "      <td>58305.0</td>\n",
       "      <td>[1113.35, 390.9, 346.38, 252.36]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'occluded': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5418</th>\n",
       "      <td>5419</td>\n",
       "      <td>5873</td>\n",
       "      <td>11</td>\n",
       "      <td>[[0.0, 1395.36, 2464.0, 1385.51, 2464.0, 524.4...</td>\n",
       "      <td>2128869.0</td>\n",
       "      <td>[0.0, 494.82, 2464.0, 900.54]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'occluded': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5419</th>\n",
       "      <td>5420</td>\n",
       "      <td>5881</td>\n",
       "      <td>12</td>\n",
       "      <td>[[598.73, 322.86, 601.2, 334.0, 648.21, 329.05...</td>\n",
       "      <td>1975.0</td>\n",
       "      <td>[598.73, 288.23, 96.49, 66.8]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'occluded': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5420</th>\n",
       "      <td>5421</td>\n",
       "      <td>5881</td>\n",
       "      <td>12</td>\n",
       "      <td>[[376.06, 132.36, 359.98, 144.73, 319.15, 158....</td>\n",
       "      <td>10263.0</td>\n",
       "      <td>[319.15, 132.36, 165.77, 89.06]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'occluded': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5421</th>\n",
       "      <td>5422</td>\n",
       "      <td>5882</td>\n",
       "      <td>12</td>\n",
       "      <td>[[91.53, 801.61, 92.77, 828.82, 61.84, 833.77,...</td>\n",
       "      <td>37768.0</td>\n",
       "      <td>[0.0, 795.42, 280.8, 214.01]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'occluded': False}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5422 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  image_id  category_id   \n",
       "0        1        10           11  \\\n",
       "1        2        14            4   \n",
       "2        3        15           11   \n",
       "3        4        15            4   \n",
       "4        5        15            4   \n",
       "...    ...       ...          ...   \n",
       "5417  5418      5872           12   \n",
       "5418  5419      5873           11   \n",
       "5419  5420      5881           12   \n",
       "5420  5421      5881           12   \n",
       "5421  5422      5882           12   \n",
       "\n",
       "                                           segmentation       area   \n",
       "0     [[0.0, 425.2, 319.15, 403.27, 682.85, 415.64, ...   160258.0  \\\n",
       "1     [[806.55, 38.64, 854.4, 34.01, 880.64, 64.88, ...     7664.0   \n",
       "2     [[84.11, 185.55, 145.96, 184.31, 174.42, 145.9...     2652.0   \n",
       "3     [[947.02, 407.55, 1002.58, 407.55, 1021.11, 44...     8810.0   \n",
       "4     [[718.57, 6.23, 754.07, 47.9, 794.2, 89.58, 82...    18234.0   \n",
       "...                                                 ...        ...   \n",
       "5417  [[1113.35, 643.26, 1459.73, 640.79, 1459.73, 5...    58305.0   \n",
       "5418  [[0.0, 1395.36, 2464.0, 1385.51, 2464.0, 524.4...  2128869.0   \n",
       "5419  [[598.73, 322.86, 601.2, 334.0, 648.21, 329.05...     1975.0   \n",
       "5420  [[376.06, 132.36, 359.98, 144.73, 319.15, 158....    10263.0   \n",
       "5421  [[91.53, 801.61, 92.77, 828.82, 61.84, 833.77,...    37768.0   \n",
       "\n",
       "                                  bbox  iscrowd           attributes  \n",
       "0         [0.0, 341.42, 2464.0, 98.97]        0  {'occluded': False}  \n",
       "1      [806.55, 34.01, 422.93, 290.19]        0  {'occluded': False}  \n",
       "2        [84.11, 139.78, 90.31, 45.77]        0  {'occluded': False}  \n",
       "3     [947.02, 407.55, 284.01, 321.05]        0  {'occluded': False}  \n",
       "4       [718.57, 3.14, 327.23, 395.15]        0  {'occluded': False}  \n",
       "...                                ...      ...                  ...  \n",
       "5417  [1113.35, 390.9, 346.38, 252.36]        0  {'occluded': False}  \n",
       "5418     [0.0, 494.82, 2464.0, 900.54]        0  {'occluded': False}  \n",
       "5419     [598.73, 288.23, 96.49, 66.8]        0  {'occluded': False}  \n",
       "5420   [319.15, 132.36, 165.77, 89.06]        0  {'occluded': False}  \n",
       "5421      [0.0, 795.42, 280.8, 214.01]        0  {'occluded': False}  \n",
       "\n",
       "[5422 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segment2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
